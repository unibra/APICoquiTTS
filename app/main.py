from fastapi import FastAPI, HTTPException
from fastapi import File, UploadFile, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import io
import logging
from typing import Optional
import uvicorn
import torch
import os
import psutil
import threading
import time
import tempfile
import shutil
from pathlib import Path

try:
    from TTS.api import TTS
    import GPUtil
except ImportError:
    TTS = None
    GPUtil = None

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√µes de otimiza√ß√£o para RTX 5090
def setup_gpu_optimization():
    """Configurar otimiza√ß√µes espec√≠ficas para RTX 5090"""
    if torch.cuda.is_available():
        try:
            # Obter informa√ß√µes da GPU
            gpu_info = torch.cuda.get_device_properties(0)
            compute_capability = f"sm_{gpu_info.major}{gpu_info.minor}"
            
            logger.info(f"GPU detectada: {gpu_info.name}")
            logger.info(f"Mem√≥ria GPU: {gpu_info.total_memory / 1024**3:.1f} GB")
            logger.info(f"Compute Capability: {gpu_info.major}.{gpu_info.minor} ({compute_capability})")
            logger.info(f"CUDA Version: {torch.version.cuda}")
            logger.info(f"PyTorch CUDA Version: {torch.version.cuda}")
            logger.info(f"CUDA Runtime API Version: {torch.cuda.get_device_capability(0)}")
            
            # Verificar se a arquitetura √© suportada pelo PyTorch
            supported_archs = torch.cuda.get_arch_list()
            logger.info(f"Arquiteturas CUDA suportadas pelo PyTorch: {supported_archs}")
            
            # RTX 5090 espec√≠fica (sm_120) - verifica√ß√£o especial
            if gpu_info.major >= 12:  # Ada Lovelace Next-gen (RTX 5090)
                logger.info("üöÄ RTX 5090 detectada! Aplicando otimiza√ß√µes espec√≠ficas...")
                
                # Configura√ß√µes espec√≠ficas para RTX 5090
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
                torch.backends.cuda.enable_flash_sdp(True)
                # Otimiza√ß√µes CUDA 12.8 espec√≠ficas
                if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
                    torch.backends.cuda.enable_flash_sdp(True)
                    logger.info("‚ö° Flash Attention habilitado")
                
                # Tensor Cores de 4¬™ gera√ß√£o para RTX 5090
                torch.set_float32_matmul_precision('high')  
                
                torch.set_float32_matmul_precision('high')  # Usar Tensor Cores de 4¬™ gera√ß√£o
                
                # Configura√ß√µes de mem√≥ria otimizadas para 32GB
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,roundup_power2_divisions:True,garbage_collection_threshold:0.6'
                os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Performance m√°xima
                os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'
                os.environ['CUDA_MODULE_LOADING'] = 'LAZY'  # CUDA 12.8 lazy loading
                
                logger.info("‚ö° Tensor Cores de 4¬™ gera√ß√£o ativados (CUDA 12.8)")
                logger.info("üß† Otimiza√ß√µes de mem√≥ria 32GB aplicadas")
                logger.info("üî• CUDA 12.8 lazy loading habilitado")
            else:
                # Para GPUs mais antigas, verificar compatibilidade normal
                arch_supported = any(compute_capability in arch for arch in supported_archs)
                if not arch_supported:
                    logger.warning(f"‚ö†Ô∏è  Arquitetura {compute_capability} pode n√£o estar suportada")
                    # Tentar mesmo assim - PyTorch nightly pode suportar
            
            # Testar uma opera√ß√£o simples na GPU para verificar compatibilidade real
            try:
                test_tensor = torch.tensor([1.0]).cuda()
                result = test_tensor * 2
                result.cpu()  # Mover de volta para CPU
                del test_tensor, result
                torch.cuda.empty_cache()
                logger.info("‚úÖ Teste CUDA bem-sucedido!")
            except Exception as cuda_test_error:
                logger.error(f"‚ùå Teste CUDA falhou: {cuda_test_error}")
                logger.warning("üîÑ Fazendo fallback para CPU")
                return False
            
            # Configurar otimiza√ß√µes GPU se tudo estiver OK
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False  # M√°xima performance
            torch.cuda.empty_cache()
            
            # Log de status final
            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"üéØ Mem√≥ria GPU - Reservada: {memory_reserved:.1f}GB, Alocada: {memory_allocated:.1f}GB")
            
            logger.info("üöÄ Otimiza√ß√µes RTX 5090 ativadas com sucesso!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao configurar GPU: {e}")
            logger.warning("üîÑ Fazendo fallback para CPU")
            return False
    return False

# Configurar GPU na inicializa√ß√£o
gpu_available = setup_gpu_optimization()

app = FastAPI(
    title="Coqui TTS API",
    description="Servi√ßo de Text-to-Speech usando Coqui TTS otimizado para NVIDIA RTX 5090",
    version="1.0.0"
)

# Modelo de request
class TTSRequest(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    text: str
    model_name: Optional[str] = "tts_models/multilingual/multi-dataset/xtts_v2"
    speaker: Optional[str] = None
    language: Optional[str] = "pt"
    use_gpu: Optional[bool] = True
    speed: Optional[float] = 1.0

# Modelo para clonagem de voz
class VoiceCloneRequest(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    text: str
    language: Optional[str] = "pt"
    use_gpu: Optional[bool] = True
# Vari√°vel global para armazenar o modelo TTS
tts_model = None

@app.on_event("startup")
async def startup_event():
    """Inicializar o modelo TTS na inicializa√ß√£o da aplica√ß√£o"""
    global tts_model
    
    if TTS is None:
        logger.error("Coqui TTS n√£o est√° instalado. Instale com: pip install TTS")
        return
    
    # Configurar vari√°veis de ambiente para aceitar licen√ßa automaticamente
    os.environ['COQUI_TOS_AGREED'] = '1'
    
    try:
        logger.info("üöÄ Inicializando modelo TTS...")
        
        # Configurar device (GPU se dispon√≠vel)
        # For√ßar CPU por enquanto devido √† incompatibilidade CUDA com RTX 5090
        device = "cpu"  # Temporary fallback
        logger.info(f"Usando device: {device}")
        
        # Lista de modelos para tentar (em ordem de prefer√™ncia)
        models_to_try = [
            ("tts_models/pt/cv/vits", "Modelo VITS portugu√™s (recomendado)"),
            ("tts_models/en/ljspeech/tacotron2-DDC", "Modelo Tacotron2 ingl√™s (est√°vel)"),
            ("tts_models/en/ljspeech/glow-tts", "Modelo GlowTTS ingl√™s (backup)")
        ]
        
        tts_model = None
        
        for model_name, description in models_to_try:
            try:
                logger.info(f"üì• Tentando carregar: {description}")
                logger.info(f"‚è≥ Modelo: {model_name}")
                
                temp_model = TTS(model_name=model_name, progress_bar=True).to(device)
                
                # Testar o modelo com uma frase simples
                test_text = "Ol√°" if "pt" in model_name else "Hello"
            # Configurar device com otimiza√ß√µes RTX 5090
            if device == "cuda" and gpu_available:
                temp_model = TTS(model_name=model_name, progress_bar=True, gpu=True).to(device)
                logger.info("üöÄ Modelo carregado na RTX 5090 com otimiza√ß√µes GPU")
            else:
                temp_model = TTS(model_name=model_name, progress_bar=True, gpu=False).to(device)
                logger.info("üñ•Ô∏è  Modelo carregado na CPU")
                
                # Verificar se o modelo funciona
                try:
                    # Verificar se √© multi-speaker
                    speakers = getattr(temp_model, 'speakers', None)
                    is_multi_speaker = speakers is not None and len(speakers) > 0
                    
                    logger.info(f"üîç Multi-speaker: {is_multi_speaker}")
                    if is_multi_speaker:
                        logger.info(f"üé§ Speakers dispon√≠veis: {speakers[:5]}...")  # Mostrar apenas os primeiros 5
                    
                    # Teste b√°sico
                    if is_multi_speaker and speakers:
                        test_speaker = speakers[0]
                        logger.info(f"üé§ Testando com speaker: {test_speaker}")
                        test_wav = temp_model.tts(text=test_text, speaker=test_speaker)
                    else:
                        test_wav = temp_model.tts(text=test_text)
                    
                    if test_wav is not None and len(test_wav) > 0:
                        logger.info(f"‚úÖ Modelo funcionando! Tamanho do √°udio: {len(test_wav)}")
                        tts_model = temp_model
                        break
                    else:
                        logger.warning("‚ö†Ô∏è  Modelo n√£o gerou √°udio v√°lido")
                        
                except Exception as test_error:
                    logger.error(f"‚ùå Teste do modelo falhou: {test_error}")
                    continue
                    
            except Exception as model_error:
                logger.error(f"‚ùå Falha ao carregar {model_name}: {model_error}")
                continue
        
        if tts_model is None:
            logger.error("‚ùå Nenhum modelo TTS p√¥de ser carregado!")
            return
            
        # Obter informa√ß√µes do modelo carregado
        model_name = getattr(tts_model, 'model_name', 'Unknown')
        speakers = getattr(tts_model, 'speakers', None)
        is_multi_speaker = speakers is not None and len(speakers) > 0
        
        logger.info(f"üéâ Modelo TTS carregado com sucesso!")
        logger.info(f"üìù Modelo: {model_name}")
        logger.info(f"üñ•Ô∏è  Device: {device}")
        logger.info(f"üé§ Multi-speaker: {is_multi_speaker}")
        
        # Log espec√≠fico para RTX 5090
        if device == "cuda" and gpu_available:
            try:
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"üéØ GPU RTX 5090 - Mem√≥ria usada: {memory_used:.1f}GB/{memory_total:.1f}GB")
            except:
                pass
        
        if is_multi_speaker:
            logger.info(f"üîä Total de speakers: {len(speakers)}")
            logger.info(f"üéµ Primeiros speakers: {speakers[:10]}")
        
        try:
            # Verificar capacidades do modelo
            languages = getattr(tts_model, 'languages', None)
            if languages:
                logger.info(f"üåê Idiomas suportados: {languages}")
        except:
            pass
            
        if device == "cuda":
            logger.info("üöÄ Usando RTX 5090 para processamento TTS acelerado!")
        else:
            logger.info("üñ•Ô∏è  Usando CPU para processamento TTS")
        
    except Exception as e:

@app.get("/")
async def root():
    """Endpoint raiz com informa√ß√µes sobre a API"""
    return {
        "message": "Coqui TTS API est√° funcionando!",
        "docs": "/docs",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """Verifica√ß√£o de sa√∫de da aplica√ß√£o"""
    gpu_info = {}
    if gpu_available and GPUtil:
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_info = {
                    "name": gpu.name,
                    "memory_used": f"{gpu.memoryUsed}MB",
                    "memory_total": f"{gpu.memoryTotal}MB",
                    "gpu_load": f"{gpu.load*100:.1f}%",
                    "temperature": f"{gpu.temperature}¬∞C"
                }
        except Exception as e:
            logger.warning(f"Erro ao obter informa√ß√µes da GPU: {e}")
    
    return {
        "status": "healthy",
        "tts_available": tts_model is not None,
        "gpu_available": gpu_available,
        "device": "cuda" if gpu_available else "cpu",
        "gpu_info": gpu_info
    }

@app.get("/models")
async def list_models():
    """Listar modelos TTS dispon√≠veis"""
    if TTS is None:
        raise HTTPException(status_code=500, detail="Coqui TTS n√£o est√° dispon√≠vel")
    
    try:
        logger.info("Tentando listar modelos TTS dispon√≠veis...")
        # Criar inst√¢ncia tempor√°ria para acessar lista de modelos
        temp_tts = TTS()
        models = temp_tts.list_models()
        logger.info(f"Encontrados {len(models) if models else 0} modelos")
        return {"available_models": models or []}
    except Exception as e:
        logger.error(f"Erro ao listar modelos: {e}", exc_info=True)
        # Fallback com modelos conhecidos - priorizando XTTS v2 e portugu√™s brasileiro
        fallback_models = [
            "tts_models/multilingual/multi-dataset/xtts_v2",
            "tts_models/pt/cv/vits",
            "tts_models/en/ljspeech/tacotron2-DDC",
            "tts_models/en/ljspeech/glow-tts",
            "tts_models/pt/cv/tacotron2-DDC",
            "tts_models/fr/mai/tacotron2-DDC",
            "tts_models/es/mai/tacotron2-DDC",
            "tts_models/de/mai/tacotron2-DDC"
        ]
        logger.info(f"Retornando lista de modelos conhecidos como fallback: {len(fallback_models)} modelos")
        return {
            "available_models": fallback_models,
            "note": "Lista de fallback - alguns modelos podem n√£o estar dispon√≠veis",
            "error": str(e)
        }

@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """
    Converter texto em √°udio usando Coqui TTS
    
    - **text**: Texto a ser convertido em √°udio
    - **model_name**: Nome do modelo TTS a ser usado (opcional)
    - **speaker**: Nome do speaker/voz (opcional, depende do modelo)
    - **language**: C√≥digo do idioma (opcional)
    """
    if TTS is None:
        raise HTTPException(status_code=500, detail="Coqui TTS n√£o est√° dispon√≠vel")
    
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="Texto n√£o pode estar vazio")
    
    try:
        # Usar modelo global ou carregar um novo se especificado
        current_tts = tts_model
        if request.model_name and request.model_name != "tts_models/multilingual/multi-dataset/xtts_v2":
            logger.info(f"Carregando modelo espec√≠fico: {request.model_name}")
            device = "cuda" if (gpu_available and request.use_gpu) else "cpu"
            current_tts = TTS(model_name=request.model_name).to(device)
        
        if current_tts is None:
            raise HTTPException(status_code=500, detail="Modelo TTS n√£o est√° carregado")
        
        # Gerar √°udio
        device = "cuda" if (gpu_available and request.use_gpu) else "cpu"
        logger.info(f"Gerando √°udio para texto: {request.text[:50]}... (device: {device})")
        
        # Medir tempo de infer√™ncia
        start_time = time.time()
        
        # Verificar se o modelo suporta m√∫ltiplos speakers
        try:
            # Tentar obter informa√ß√µes sobre speakers do modelo
            speakers = getattr(current_tts, 'speakers', None)
            is_multi_speaker = speakers is not None and len(speakers) > 0
            
            # Gerar √°udio com par√¢metros apropriados
            tts_kwargs = {"text": request.text}
            
            # Adicionar speaker apenas se o modelo suportar e foi especificado
            if is_multi_speaker and request.speaker:
                tts_kwargs["speaker"] = request.speaker
                logger.info(f"Usando speaker: {request.speaker}")
            
            # Adicionar language se especificado
            if request.language:
                tts_kwargs["language"] = request.language
                logger.info(f"Usando idioma: {request.language} (Portugu√™s do Brasil para 'pt')")
            
            logger.info(f"Par√¢metros TTS: {tts_kwargs}")
            wav_data = current_tts.tts(**tts_kwargs)
            
        except Exception as tts_error:
            logger.error(f"Erro espec√≠fico do TTS: {tts_error}")
            # Fallback - tentar apenas com texto
            logger.info("Tentando fallback apenas com texto...")
            wav_data = current_tts.tts(text=request.text)
        
        inference_time = time.time() - start_time
        logger.info(f"√Åudio gerado em {inference_time:.2f} segundos")
        
        # Criar buffer em mem√≥ria para o √°udio
        audio_buffer = io.BytesIO()
        
        # Converter para bytes e escrever no buffer
        import soundfile as sf
        import numpy as np
        
        # Converter para numpy array se necess√°rio
        if not isinstance(wav_data, np.ndarray):
            wav_data = np.array(wav_data)
        
        # Escrever no buffer como WAV
        sf.write(audio_buffer, wav_data, 22050, format='WAV')
        audio_buffer.seek(0)
        
        logger.info("√Åudio gerado com sucesso!")
        
        # Retornar como streaming response
        return StreamingResponse(
            io.BytesIO(audio_buffer.read()),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=tts_output.wav"}
        )
    except Exception as e:
        logger.error(f"Erro ao gerar √°udio: {e}")
        raise HTTPException(status_code=500, detail=f"Erro ao gerar √°udio: {str(e)}")

@app.post("/tts-simple")
async def simple_text_to_speech(text: str):
    """
    Vers√£o simplificada do endpoint TTS - apenas recebe texto como par√¢metro
    """
    request = TTSRequest(text=text)
    return await text_to_speech(request)

@app.post("/tts-clone")
async def voice_cloning_tts(
    text: str = Form(...),
    language: str = Form("pt"),
    use_gpu: bool = Form(True),
    speaker_audio: UploadFile = File(...)
):
    """
    üé≠ S√≠ntese de Voz com √Åudio de Refer√™ncia
    
    Gera √°udio usando o modelo TTS dispon√≠vel. Se poss√≠vel, usa o √°udio como refer√™ncia.
    
    - **text**: Texto a ser convertido em √°udio
    - **language**: C√≥digo do idioma (pt para Portugu√™s do Brasil)
    - **use_gpu**: Usar GPU para processamento (se dispon√≠vel)
    - **speaker_audio**: Arquivo de √°udio (WAV, MP3, etc.)
    
    üìã Recomenda√ß√µes para o √°udio:
    - Dura√ß√£o: 6-12 segundos (ideal)
    - Qualidade: Limpo, sem ru√≠do
    - Formato: WAV, MP3, M4A, FLAC
    - Conte√∫do: Uma pessoa falando claramente
    """
    logger.info(f"üéµ Iniciando s√≠ntese TTS - Texto: '{text[:50]}...', Idioma: {language}")
    
    if not text.strip():
        raise HTTPException(status_code=400, detail="Texto n√£o pode estar vazio")
    
    if TTS is None:
        raise HTTPException(status_code=500, detail="Coqui TTS n√£o est√° dispon√≠vel")
    
    # Verificar se temos um modelo carregado
    if tts_model is None:
        raise HTTPException(status_code=500, detail="Modelo TTS n√£o est√° carregado. Tente novamente em alguns segundos.")
    
    # Obter informa√ß√µes do modelo
    model_name = getattr(tts_model, 'model_name', 'Unknown')
    speakers = getattr(tts_model, 'speakers', None)
    is_multi_speaker = speakers is not None and len(speakers) > 0
    
    logger.info(f"ü§ñ Usando modelo: {model_name}")
    logger.info(f"üé§ Multi-speaker: {is_multi_speaker}")
    
    try:
        # Medir tempo de infer√™ncia
        start_time = time.time()
        
        # Estrat√©gia: Tentar diferentes abordagens baseadas no modelo
        wav_data = None
        method_used = "unknown"
        
        # M√©todo 1: Se √© multi-speaker, usar um speaker padr√£o
        if is_multi_speaker and speakers:
            try:
                # Selecionar speaker padr√£o (primeiro da lista)
                default_speaker = speakers[0]
                logger.info(f"üé§ Tentativa 1: Multi-speaker com '{default_speaker}'")
                
                # Decidir se incluir language baseado no modelo
                if "pt" in model_name.lower() or "multilingual" in model_name.lower():
                    # Modelo portugu√™s ou multilingual
                    wav_data = tts_model.tts(text=text, speaker=default_speaker, language=language)
                    method_used = f"multi-speaker com language ({default_speaker})"
                else:
                    # Modelo ingl√™s - sem language
                    wav_data = tts_model.tts(text=text, speaker=default_speaker)
                    method_used = f"multi-speaker sem language ({default_speaker})"
                    
                logger.info(f"‚úÖ M√©todo 1 funcionou: {method_used}")
                
            except Exception as method1_error:
                logger.warning(f"‚ö†Ô∏è  M√©todo 1 falhou: {method1_error}")
        
        # M√©todo 2: TTS simples (se m√©todo 1 falhou ou modelo n√£o √© multi-speaker)
        if wav_data is None:
            try:
                logger.info("üéµ Tentativa 2: TTS simples")
                
                # Decidir se incluir language baseado no modelo
                if "pt" in model_name.lower() or "multilingual" in model_name.lower():
                    wav_data = tts_model.tts(text=text, language=language)
                    method_used = "TTS simples com language"
                else:
                    wav_data = tts_model.tts(text=text)
                    method_used = "TTS simples sem language"
                    
                logger.info(f"‚úÖ M√©todo 2 funcionou: {method_used}")
                
            except Exception as method2_error:
                logger.error(f"‚ùå M√©todo 2 tamb√©m falhou: {method2_error}")
                raise Exception(f"Todos os m√©todos falharam. √öltimo erro: {method2_error}")
        
        # Criar buffer em mem√≥ria para o √°udio
        logger.info("üíæ Convertendo √°udio para formato WAV...")
        audio_buffer = io.BytesIO()
        
        # Converter para bytes e escrever no buffer
        import soundfile as sf
        import numpy as np
        
        # Converter para numpy array se necess√°rio
        if not isinstance(wav_data, np.ndarray):
            logger.info("üîÑ Convertendo dados para numpy array...")
            wav_data = np.array(wav_data)
        
        # Verificar se temos dados v√°lidos
        if len(wav_data) == 0:
            raise Exception("Dados de √°udio est√£o vazios ap√≥s convers√£o")
        
        logger.info(f"üìä Array final: shape={wav_data.shape}, dtype={wav_data.dtype}, m√©todo={method_used}")
        
        # Escrever no buffer como WAV
        sample_rate = 22050  # Taxa de amostragem padr√£o do XTTS v2
        sf.write(audio_buffer, wav_data, sample_rate, format='WAV')
        audio_buffer.seek(0)
        
        # Verificar tamanho do buffer
        buffer_size = audio_buffer.getbuffer().nbytes
        logger.info(f"üíæ Buffer de √°udio criado: {buffer_size} bytes")
        
        if buffer_size == 0:
            raise Exception("Buffer de √°udio est√° vazio")
        
        logger.info(f"üéâ √Åudio TTS gerado com sucesso usando: {method_used}! üéµ")
        
        # Retornar como streaming response
        return StreamingResponse(
            io.BytesIO(audio_buffer.read()),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=voice_clone_output.wav"}
        )
        
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        logger.error(f"‚ùå Erro ao gerar √°udio TTS: {error_msg}")
        logger.error(f"üìç Detalhes do erro:", exc_info=True)

@app.get("/clone-info")
async def voice_clone_info():
    """
    üìñ Informa√ß√µes sobre s√≠ntese TTS
    """
    return {
        "feature": "S√≠ntese de Voz TTS",
        "description": "Converte texto em √°udio usando modelos Coqui TTS otimizados",
        "endpoint": "/tts-clone",
        "method": "POST",
        "model_info": {
            "loaded_model": getattr(tts_model, 'model_name', 'N√£o carregado') if tts_model else 'N√£o carregado',
            "multi_speaker": bool(getattr(tts_model, 'speakers', None)) if tts_model else False,
            "device": "cpu",
            "status": "funcionando" if tts_model else "n√£o inicializado"
        },
        "supported_languages": ["pt", "en", "es", "fr", "de"],
        "tips": [
            "Modelos portugueses funcionam melhor para texto em portugu√™s",
            "Arquivos de √°udio s√£o aceitos mas podem n√£o influenciar o resultado",
            "Use textos claros e bem pontuados para melhores resultados",
            "O sistema usa automaticamente a melhor voz dispon√≠vel"
        ]
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8888)