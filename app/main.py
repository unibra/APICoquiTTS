from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import io
import logging
from typing import Optional
import uvicorn
import torch
import os
import psutil
import time
import numpy as np
import soundfile as sf

# === Configurar logging ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# === Vari√°veis globais de controle ===
TTS = None
TTS_AVAILABLE = False
GPU_UTILS_AVAILABLE = False
tts_model = None
device_type = "cpu"
gpu_available = False

# === Importar GPUtil opcionalmente ===
try:
    import GPUtil
    GPU_UTILS_AVAILABLE = True
    logger.info("‚úÖ GPUtil importado com sucesso")
except ImportError:
    logger.warning("‚ö†Ô∏è  GPUtil n√£o dispon√≠vel - monitoramento GPU limitado")
    GPUtil = None

def setup_gpu_optimization():
    """
    Configurar otimiza√ß√µes GPU para CUDA 12.1
    Retorna True se GPU estiver funcionando, False para fallback CPU
    """
    global gpu_available, device_type
    
    try:
        logger.info("üîç Verificando disponibilidade CUDA...")
        
        # Verifica√ß√£o b√°sica de CUDA
        if not torch.cuda.is_available():
            logger.info("üñ•Ô∏è  CUDA n√£o dispon√≠vel no PyTorch")
            return False
            
        # Verificar se h√° GPUs dispon√≠veis
        device_count = torch.cuda.device_count()
        if device_count == 0:
            logger.info("üñ•Ô∏è  Nenhuma GPU CUDA encontrada")
            return False
            
        logger.info(f"üéØ {device_count} GPU(s) CUDA detectada(s)")
        
        try:
            # Tentar obter informa√ß√µes da GPU (pode falhar com "stoi" error)
            gpu_info = torch.cuda.get_device_properties(0)
            logger.info(f"üéØ GPU: {gpu_info.name}")
            logger.info(f"üíæ Mem√≥ria GPU: {gpu_info.total_memory / 1024**3:.1f} GB")
            logger.info(f"‚ö° Compute Capability: {gpu_info.major}.{gpu_info.minor}")
            
        except RuntimeError as e:
            if "stoi" in str(e):
                logger.warning("‚ö†Ô∏è  Erro CUDA 'stoi' detectado - problema comum com runtime")
                logger.info("üîÑ Tentando continuar com CUDA b√°sico...")
            else:
                logger.error(f"‚ùå Erro CUDA espec√≠fico: {e}")
                logger.warning("üîÑ Fazendo fallback para CPU")
                return False
        
        # Testar opera√ß√£o b√°sica CUDA
        try:
            logger.info("üß™ Testando opera√ß√£o CUDA...")
            test_tensor = torch.tensor([1.0]).cuda()
            result = test_tensor * 2
            result.cpu()
            del test_tensor, result
            torch.cuda.empty_cache()
            logger.info("‚úÖ Teste CUDA bem-sucedido!")
            
        except Exception as cuda_test_error:
            logger.error(f"‚ùå Teste CUDA falhou: {cuda_test_error}")
            logger.warning("üîÑ Fazendo fallback para CPU")
            return False
        
        # Aplicar otimiza√ß√µes
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision('high')
        torch.cuda.empty_cache()
        
        logger.info("üöÄ GPU configurada e funcionando!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro na configura√ß√£o GPU: {e}")
        logger.warning("üîÑ Fazendo fallback para CPU")
        return False

# === Configurar GPU na inicializa√ß√£o ===
gpu_available = setup_gpu_optimization()
device_type = "cuda" if gpu_available else "cpu"

logger.info("=" * 60)
logger.info("üéØ CONFIGURA√á√ÉO FINAL DO SISTEMA")
logger.info("=" * 60)
logger.info(f"üñ•Ô∏è  Device: {device_type.upper()}")
logger.info(f"üéµ TTS: Aguardando carregamento...")
if gpu_available:
    logger.info("üöÄ Modo: GPU Acelerada (CUDA 12.1)")
else:
    logger.info("üîÑ Modo: CPU Fallback")
logger.info("=" * 60)

# === Inicializar FastAPI ===
app = FastAPI(
    title="Coqui TTS API",
    description="Servi√ßo de Text-to-Speech usando Coqui TTS com CUDA 12.1",
    version="2.0.0"
)

# === Modelos de Request ===
class TTSRequest(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    text: str
    model_name: Optional[str] = None
    speaker: Optional[str] = None
    language: Optional[str] = "pt"
    use_gpu: Optional[bool] = True
    speed: Optional[float] = 1.0

@app.on_event("startup")
async def startup_event():
    """Inicializar o modelo TTS na inicializa√ß√£o"""
    global TTS, TTS_AVAILABLE, tts_model
    
    logger.info("üöÄ Iniciando aplica√ß√£o TTS...")
    
    # === Tentar importar TTS ===
    try:
        logger.info("üì¶ Tentando importar Coqui TTS...")
        from TTS.api import TTS as TTS_Class
        TTS = TTS_Class
        TTS_AVAILABLE = True
        logger.info("‚úÖ Coqui TTS importado com sucesso!")
        
    except ImportError as e:
        logger.error(f"‚ùå Falha ao importar TTS: {e}")
        logger.error("üí° Solu√ß√£o: docker exec -it tts-api pip install TTS")
        TTS_AVAILABLE = False
        return
    
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao importar TTS: {e}")
        TTS_AVAILABLE = False
        return
    
    # === Configurar licen√ßa ===
    os.environ['COQUI_TOS_AGREED'] = '1'
    
    # === Tentar carregar modelo ===
    try:
        logger.info(f"üéØ Carregando modelo TTS no device: {device_type}")
        
        # Lista de modelos para tentar
        models_to_try = [
            ("tts_models/pt/cv/vits", "Modelo VITS portugu√™s (recomendado)"),
            ("tts_models/en/ljspeech/tacotron2-DDC", "Modelo Tacotron2 ingl√™s"),
            ("tts_models/en/ljspeech/glow-tts", "Modelo GlowTTS ingl√™s (backup)")
        ]
        
        for model_name, description in models_to_try:
            try:
                logger.info(f"üì• Tentando: {description}")
                
                if gpu_available:
                    temp_model = TTS(model_name=model_name, progress_bar=True, gpu=True)
                    logger.info("üöÄ Carregando modelo na GPU...")
                else:
                    temp_model = TTS(model_name=model_name, progress_bar=True, gpu=False)
                    logger.info("üñ•Ô∏è  Carregando modelo na CPU...")
                
                # Teste b√°sico
                test_text = "Ol√°" if "pt" in model_name else "Hello"
                test_wav = temp_model.tts(text=test_text)
                
                if test_wav is not None and len(test_wav) > 0:
                    logger.info(f"‚úÖ Modelo funcionando! Audio: {len(test_wav)} samples")
                    tts_model = temp_model
                    logger.info(f"üéâ Modelo carregado: {model_name}")
                    break
                    
            except Exception as model_error:
                logger.error(f"‚ùå Falha em {model_name}: {model_error}")
                continue
        
        if tts_model is None:
            logger.error("‚ùå Nenhum modelo TTS p√¥de ser carregado!")
            return
        
        # === Informa√ß√µes finais ===
        model_name = getattr(tts_model, 'model_name', 'Unknown')
        speakers = getattr(tts_model, 'speakers', None)
        
        logger.info("üéâ TTS inicializado com sucesso!")
        logger.info(f"üìù Modelo: {model_name}")
        logger.info(f"üñ•Ô∏è  Device: {device_type}")
        
        if speakers and len(speakers) > 0:
            logger.info(f"üé§ Speakers: {len(speakers)} dispon√≠veis")
        
        if gpu_available:
            try:
                memory_used = torch.cuda.memory_allocated(0) / 1024**3
                logger.info(f"üéØ GPU - Mem√≥ria em uso: {memory_used:.1f}GB")
            except:
                logger.info("üéØ GPU ativa")
        else:
            cpu_count = psutil.cpu_count()
            memory_info = psutil.virtual_memory()
            logger.info(f"üñ•Ô∏è  CPU: {cpu_count} cores, {memory_info.available / 1024**3:.1f}GB RAM dispon√≠vel")
            
    except Exception as e:
        logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}", exc_info=True)
        logger.warning("‚ö†Ô∏è  Aplica√ß√£o iniciar√° sem modelo TTS")

@app.get("/")
async def root():
    """Endpoint raiz"""
    return {
        "message": "Coqui TTS API v2.0 - Refatorada e Otimizada",
        "docs": "/docs",
        "device": device_type,
        "status": "operational"
    }

@app.get("/health")
async def health_check():
    """Verifica√ß√£o de sa√∫de"""
    gpu_info = {}
    
    if gpu_available and GPU_UTILS_AVAILABLE and GPUtil:
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_info = {
                    "name": gpu.name,
                    "memory_used": f"{gpu.memoryUsed}MB",
                    "memory_total": f"{gpu.memoryTotal}MB",
                    "gpu_load": f"{gpu.load*100:.1f}%",
                    "temperature": f"{gpu.temperature}¬∞C"
                }
        except Exception as e:
            logger.warning(f"Erro ao obter info GPU: {e}")
    
    return {
        "status": "healthy",
        "tts_installed": TTS_AVAILABLE,
        "tts_model_loaded": tts_model is not None,
        "gpu_available": gpu_available,
        "device": device_type,
        "mode": "GPU Accelerated" if gpu_available else "CPU Fallback",
        "pytorch_version": torch.__version__,
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        "gpu_info": gpu_info
    }

@app.get("/models")
async def list_models():
    """Listar modelos dispon√≠veis"""
    if not TTS_AVAILABLE:
        raise HTTPException(status_code=500, detail="TTS n√£o dispon√≠vel")
    
    try:
        temp_tts = TTS()
        models = temp_tts.list_models()
        return {"available_models": models or []}
    except Exception as e:
        logger.error(f"Erro ao listar modelos: {e}")
        fallback_models = [
            "tts_models/pt/cv/vits",
            "tts_models/en/ljspeech/tacotron2-DDC",
            "tts_models/en/ljspeech/glow-tts",
            "tts_models/multilingual/multi-dataset/xtts_v2"
        ]
        return {
            "available_models": fallback_models,
            "note": "Lista de fallback",
            "error": str(e)
        }

@app.post("/tts")
async def text_to_speech(request: TTSRequest):
    """Converter texto em √°udio"""
    if not TTS_AVAILABLE or tts_model is None:
        raise HTTPException(status_code=500, detail="TTS n√£o dispon√≠vel")
    
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="Texto vazio")
    
    try:
        logger.info(f"üéµ Gerando √°udio: {request.text[:50]}...")
        
        start_time = time.time()
        
        # Usar modelo global ou carregar espec√≠fico
        current_tts = tts_model
        if request.model_name:
            logger.info(f"üì¶ Carregando modelo espec√≠fico: {request.model_name}")
            use_gpu = gpu_available and request.use_gpu
            current_tts = TTS(model_name=request.model_name, gpu=use_gpu)
        
        # Gerar √°udio
        tts_kwargs = {"text": request.text}
        
        # Adicionar par√¢metros opcionais
        if request.speaker:
            tts_kwargs["speaker"] = request.speaker
        if request.language:
            tts_kwargs["language"] = request.language
        
        wav_data = current_tts.tts(**tts_kwargs)
        
        inference_time = time.time() - start_time
        logger.info(f"‚úÖ √Åudio gerado em {inference_time:.2f}s")
        
        # Converter para WAV
        if not isinstance(wav_data, np.ndarray):
            wav_data = np.array(wav_data)
        
        audio_buffer = io.BytesIO()
        sf.write(audio_buffer, wav_data, 22050, format='WAV')
        audio_buffer.seek(0)
        
        return StreamingResponse(
            io.BytesIO(audio_buffer.read()),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=tts_output.wav"}
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar √°udio: {e}")
        raise HTTPException(status_code=500, detail=f"Erro TTS: {str(e)}")

@app.post("/tts-simple")
async def simple_tts(text: str):
    """Endpoint TTS simplificado"""
    request = TTSRequest(text=text)
    return await text_to_speech(request)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8888)